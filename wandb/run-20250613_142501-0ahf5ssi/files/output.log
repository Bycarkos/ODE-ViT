Config of the encoder: <class 'transformers.models.vit.modeling_vit.ViTModel'> is overwritten by shared encoder config: ViTConfig {
  "attention_probs_dropout_prob": 0.0,
  "encoder_stride": 16,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.0,
  "hidden_size": 768,
  "image_size": 384,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "model_type": "vit",
  "num_attention_heads": 12,
  "num_channels": 3,
  "num_hidden_layers": 12,
  "patch_size": 16,
  "pooler_act": "tanh",
  "pooler_output_size": 768,
  "qkv_bias": false,
  "torch_dtype": "float32",
  "transformers_version": "4.51.1"
}

Config of the decoder: <class 'transformers.models.trocr.modeling_trocr.TrOCRForCausalLM'> is overwritten by shared decoder config: TrOCRConfig {
  "activation_dropout": 0.0,
  "activation_function": "gelu",
  "add_cross_attention": true,
  "attention_dropout": 0.0,
  "bos_token_id": 0,
  "classifier_dropout": 0.0,
  "cross_attention_hidden_size": 768,
  "d_model": 1024,
  "decoder_attention_heads": 16,
  "decoder_ffn_dim": 4096,
  "decoder_layerdrop": 0.0,
  "decoder_layers": 12,
  "decoder_start_token_id": 2,
  "dropout": 0.1,
  "eos_token_id": 2,
  "init_std": 0.02,
  "is_decoder": true,
  "layernorm_embedding": true,
  "max_position_embeddings": 512,
  "model_type": "trocr",
  "pad_token_id": 1,
  "scale_embedding": false,
  "torch_dtype": "float32",
  "transformers_version": "4.51.1",
  "use_cache": false,
  "use_learned_position_embeddings": true,
  "vocab_size": 50265
}

Some weights of VisionEncoderDecoderModel were not initialized from the model checkpoint at microsoft/trocr-base-handwritten and are newly initialized: ['encoder.pooler.dense.bias', 'encoder.pooler.dense.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
CREATING THE BASELINE METRIC VALUE
 STARTING TO EVALUATE FO THE FIRST TIME
val Procedure: 24it [00:02,  9.30it/s]
Training Procedure: 345it [08:59,  1.56s/it]                                                                            | 0/99 [00:00<?, ?it/s]
Validation Loss Epoch: 0 Value: 318.19604431152345 Optimal_loss: 318.19604431152345
val Procedure: 24it [00:02, 10.38it/s]4s/it]
Loss Epoch: 1 Value: 124.68452289277229
Training Procedure: 345it [08:54,  1.55s/it]                                                                | 1/99 [09:03<14:47:16, 543.23s/it]
Model Updated: Validation Loss Epoch: 0 Value: 8.905110607147217 Optimal_loss: 8.905110607147217
val Procedure: 24it [00:02, 10.45it/s]4s/it]
Loss Epoch: 2 Value: 1.0083302335678668
Training Procedure: 345it [08:54,  1.55s/it]                                                                | 2/99 [18:00<14:32:54, 539.94s/it]
Model Updated: Validation Loss Epoch: 0 Value: 0.721563173532486 Optimal_loss: 0.721563173532486
val Procedure: 24it [00:02, 10.52it/s]4s/it]
Loss Epoch: 3 Value: -1.298978984453108
Training Procedure: 345it [08:54,  1.55s/it]                                                                | 3/99 [26:58<14:22:25, 539.02s/it]
Model Updated: Validation Loss Epoch: 0 Value: -0.13458290696144104 Optimal_loss: -0.13458290696144104
val Procedure: 24it [00:02, 10.44it/s]4s/it]
Loss Epoch: 4 Value: -1.9496708978777346
Training Procedure: 345it [08:54,  1.55s/it]                                                                | 4/99 [35:56<14:12:36, 538.49s/it]
Model Updated: Validation Loss Epoch: 0 Value: -1.1794701766967775 Optimal_loss: -1.1794701766967775
val Procedure: 24it [00:02, 10.39it/s]4s/it]
Loss Epoch: 5 Value: -2.3700518708298173
Training Procedure: 345it [08:54,  1.55s/it]                                                                | 5/99 [44:54<14:03:07, 538.16s/it]
Model Updated: Validation Loss Epoch: 0 Value: -1.4400491166114806 Optimal_loss: -1.4400491166114806
val Procedure: 24it [00:02, 10.40it/s]4s/it]
Loss Epoch: 6 Value: -2.7582764096882033
Training Procedure: 345it [08:53,  1.55s/it]                                                                | 6/99 [53:51<13:53:31, 537.76s/it]
val Procedure: 24it [00:02, 10.41it/s]4s/it]
Loss Epoch: 7 Value: -3.037078787278438
Training Procedure: 345it [08:53,  1.55s/it]                                                              | 7/99 [1:02:48<13:44:24, 537.65s/it]
Model Updated: Validation Loss Epoch: 0 Value: -1.5796687912940979 Optimal_loss: -1.5796687912940979
val Procedure: 24it [00:02, 10.44it/s]4s/it]
Loss Epoch: 8 Value: -3.2610788445541825
Training Procedure: 345it [08:54,  1.55s/it]                                                              | 8/99 [1:11:45<13:35:14, 537.52s/it]
Model Updated: Validation Loss Epoch: 0 Value: -2.2254885768890382 Optimal_loss: -2.2254885768890382
val Procedure: 24it [00:02, 10.34it/s]4s/it]
Loss Epoch: 9 Value: -3.4292051114897797
Training Procedure: 345it [08:53,  1.55s/it]                                                              | 9/99 [1:20:43<13:26:18, 537.54s/it]
Model Updated: Validation Loss Epoch: 0 Value: -2.5019714522361753 Optimal_loss: -2.5019714522361753
val Procedure: 24it [00:02, 10.24it/s]4s/it]
Loss Epoch: 10 Value: -3.5719655962957853
Training Procedure: 345it [08:53,  1.55s/it]                                                             | 10/99 [1:29:40<13:17:00, 537.31s/it]
val Procedure: 24it [00:02, 10.35it/s]4s/it]
Loss Epoch: 11 Value: -3.6616601584614186
Training Procedure: 345it [08:53,  1.55s/it]                                                             | 11/99 [1:38:37<13:08:11, 537.41s/it]
Model Updated: Validation Loss Epoch: 0 Value: -2.6378276777267455 Optimal_loss: -2.6378276777267455
val Procedure: 24it [00:02, 10.37it/s]4s/it]
Loss Epoch: 12 Value: -3.75622605793718
Training Procedure: 345it [08:53,  1.55s/it]                                                             | 12/99 [1:47:35<12:59:15, 537.42s/it]
Model Updated: Validation Loss Epoch: 0 Value: -2.848969736099243 Optimal_loss: -2.848969736099243
val Procedure: 24it [00:02, 10.40it/s]4s/it]
Loss Epoch: 13 Value: -3.8512690392093383
Training Procedure: 345it [08:54,  1.55s/it]                                                             | 13/99 [1:56:31<12:50:01, 537.23s/it]
val Procedure: 24it [00:02, 10.44it/s]4s/it]
Loss Epoch: 14 Value: -3.9108093773109327
Training Procedure: 345it [08:54,  1.55s/it]                                                             | 14/99 [2:05:29<12:41:16, 537.37s/it]
Model Updated: Validation Loss Epoch: 0 Value: -3.4862731647491456 Optimal_loss: -3.4862731647491456
val Procedure: 24it [00:02, 10.39it/s]5s/it]
Loss Epoch: 15 Value: -3.995990757320238
Training Procedure: 345it [08:54,  1.55s/it]                                                             | 15/99 [2:14:26<12:32:13, 537.30s/it]
val Procedure: 24it [00:02, 10.22it/s]4s/it]
Loss Epoch: 16 Value: -4.063856495290563
Training Procedure: 345it [08:54,  1.55s/it]                                                             | 16/99 [2:23:23<12:23:13, 537.27s/it]
val Procedure: 24it [00:02, 10.44it/s]4s/it]
Loss Epoch: 17 Value: -4.126910166809524
Training Procedure: 345it [08:53,  1.55s/it]                                                             | 17/99 [2:32:21<12:14:18, 537.29s/it]
val Procedure: 24it [00:02, 10.39it/s]4s/it]
Loss Epoch: 18 Value: -4.180770495317985
Training Procedure: 345it [08:54,  1.55s/it]                                                             | 18/99 [2:41:18<12:05:12, 537.19s/it]
val Procedure: 24it [00:02, 10.31it/s]4s/it]
Loss Epoch: 19 Value: -4.252695975787398
Training Procedure: 345it [08:54,  1.55s/it]                                                             | 19/99 [2:50:15<11:56:19, 537.25s/it]
val Procedure: 24it [00:02, 10.38it/s]4s/it]
Loss Epoch: 20 Value: -4.296452632157699
Training Procedure: 345it [08:54,  1.55s/it]                                                             | 20/99 [2:59:13<11:47:39, 537.46s/it]
Model Updated: Validation Loss Epoch: 0 Value: -3.6617805480957033 Optimal_loss: -3.6617805480957033
val Procedure: 24it [00:02, 10.34it/s]4s/it]
Loss Epoch: 21 Value: -4.337673369697902
Training Procedure: 345it [08:54,  1.55s/it]                                                             | 21/99 [3:08:10<11:38:33, 537.35s/it]
val Procedure: 24it [00:02, 10.38it/s]4s/it]
Loss Epoch: 22 Value: -4.408277273869169
Training Procedure: 345it [08:54,  1.55s/it]                                                             | 22/99 [3:17:08<11:29:48, 537.51s/it]
Model Updated: Validation Loss Epoch: 0 Value: -3.7222795343399047 Optimal_loss: -3.7222795343399047
val Procedure: 24it [00:02, 10.32it/s]4s/it]
Loss Epoch: 23 Value: -4.4649342612943785
Training Procedure: 163it [04:14,  1.56s/it]                                                             | 23/99 [3:26:05<11:20:42, 537.40s/it]
Traceback (most recent call last):                                                                                                             
  File "/home/cboned/Projects/OCR-Koopman/main_ocr_ctc.py", line 247, in <module>
    main(cfg=cfg)
  File "/home/cboned/Projects/OCR-Koopman/main_ocr_ctc.py", line 165, in main
    _, train_loss = train_ocr_task_ctc(
  File "/home/cboned/Projects/OCR-Koopman/train.py", line 58, in train_ocr_task_ctc
    generated_ids = ctc_decoder(to_generate.detach().cpu().numpy())
KeyboardInterrupt
Exception ignored in atexit callback: <function _start_and_connect_service.<locals>.teardown_atexit at 0x7f550aaf2710>
Traceback (most recent call last):
  File "/home/cboned/miniconda3/envs/graphocr/lib/python3.10/site-packages/wandb/sdk/lib/service_connection.py", line 94, in teardown_atexit
    conn.teardown(hooks.exit_code)
  File "/home/cboned/miniconda3/envs/graphocr/lib/python3.10/site-packages/wandb/sdk/lib/service_connection.py", line 226, in teardown
    self._router.join()
  File "/home/cboned/miniconda3/envs/graphocr/lib/python3.10/site-packages/wandb/sdk/interface/router.py", line 75, in join
    self._thread.join()
  File "/home/cboned/miniconda3/envs/graphocr/lib/python3.10/threading.py", line 1096, in join
    self._wait_for_tstate_lock()
  File "/home/cboned/miniconda3/envs/graphocr/lib/python3.10/threading.py", line 1116, in _wait_for_tstate_lock
    if lock.acquire(block, timeout):
KeyboardInterrupt:
