{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1056607e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['/home/cboned/miniconda3/envs/graphocr/lib/python310.zip', '../', '/home/cboned/miniconda3/envs/graphocr/lib/python3.10', '/home/cboned/miniconda3/envs/graphocr/lib/python3.10/lib-dynload', '', '/home/cboned/miniconda3/envs/graphocr/lib/python3.10/site-packages']\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "sys.path.insert(1, '../')\n",
    "print(sys.path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f19aeb9c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "os.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\"\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"5\"\n",
    "\n",
    "\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(device)\n",
    "\n",
    "\n",
    "from collections import defaultdict\n",
    "from datasets import Collator\n",
    "\n",
    "import os\n",
    "\n",
    "from torchvision.datasets import CIFAR10, CIFAR100, ImageFolder\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "from transformers import ViTForImageClassification,  CLIPModel, ViTModel, ViTImageProcessor, AutoModel, AutoModelForImageClassification, Dinov2ForImageClassification, AutoImageProcessor\n",
    "from models.ode_transformer_gpt  import ViTNeuralODE\n",
    "import numpy as np\n",
    "import scienceplots\n",
    "\n",
    "from torchdiffeq import odeint\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn.decomposition import PCA\n",
    "from matplotlib.lines import Line2D\n",
    "import seaborn as sns\n",
    "from sklearn.manifold import TSNE\n",
    "\n",
    "from PIL import Image\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "dc12a3ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "DATASET_PATH = \"/data/users/cboned/cifar\"#imagenet100\"\n",
    "train_dataset = CIFAR100(root=DATASET_PATH, download=False, train=True) ## Modify to Cifar100\n",
    "validation_dataset = CIFAR100(root=DATASET_PATH, download=False, train=False)\n",
    "\n",
    "#train_dataset = ImageFolder(root=DATASET_PATH + \"/train\")\n",
    "#validation_dataset = ImageFolder(root=DATASET_PATH + \"/val\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ba9ff933",
   "metadata": {},
   "outputs": [],
   "source": [
    "processor = AutoImageProcessor.from_pretrained('facebook/dino-vitb16', use_fast=False)\n",
    "collator = Collator(processor)\n",
    "\n",
    "train_dloader = DataLoader(train_dataset, \n",
    "                           shuffle=False,\n",
    "                           batch_size=1,\n",
    "                           pin_memory=False,\n",
    "                           num_workers=1,\n",
    "                           collate_fn=collator.classification_collate_fn\n",
    "                           )\n",
    "\n",
    "\n",
    "test_dloader = DataLoader(validation_dataset,\n",
    "                          batch_size=1,\n",
    "                          pin_memory=False,\n",
    "                          num_workers=1,\n",
    "                          drop_last=True,\n",
    "                          shuffle=True,\n",
    "                          collate_fn=collator.classification_collate_fn\n",
    "\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "9a60f5c5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of ViTModel were not initialized from the model checkpoint at facebook/dino-vitb16 and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "model = ViTModel.from_pretrained(\"facebook/dino-vitb16\").to(device)\n",
    "model = model.eval()\n",
    "model.set_attn_implementation('eager')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "505678a5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error(s) in loading state_dict for ViTNeuralODE:\n",
      "\tsize mismatch for odefunc.block.mlp.fc1.weight: copying a param with shape torch.Size([3072, 768]) from checkpoint, the shape in current model is torch.Size([768, 768]).\n",
      "\tsize mismatch for odefunc.block.mlp.fc2.weight: copying a param with shape torch.Size([768, 3072]) from checkpoint, the shape in current model is torch.Size([768, 768]).\n",
      "Matching individual Weights\n"
     ]
    }
   ],
   "source": [
    "EDO_CHECKPOINT_PATH = \"/data/users/cboned/checkpoints/EDO_DISTILLATION_VIT_ON_CIFAR100_DINO_ATTENTION_1-Attention_L1_NO_JASMIN_add_supervision_at_the_end.pt\"\n",
    "\n",
    "edo_model = ViTNeuralODE(\n",
    "    img_size=224,\n",
    "    patch_size=16,\n",
    "    in_chans=3,\n",
    "    mlp_ratio=1,\n",
    "    num_classes=100,\n",
    "    embed_dim=768,\n",
    "    num_heads=12,\n",
    "    emulate_depth=12.0,\n",
    "    time_interval=1.0,   # match 12 \"layers\" by integrating over [0,12]\n",
    "    num_eval_steps=24,\n",
    "    solver=\"euler\",\n",
    "    register_tokens=10,\n",
    "    pos_embed_register_tokens=False\n",
    ")\n",
    "\n",
    "edo_model.patch_embed.pos_embed = (\n",
    "    model.embeddings.position_embeddings)\n",
    "\n",
    "\n",
    "try:\n",
    "    edo_model.load_state_dict(torch.load(EDO_CHECKPOINT_PATH, weights_only=True)[\"state_dict\"])\n",
    "except Exception as e:\n",
    "    print(e)\n",
    "    print(\"Matching individual Weights\")\n",
    "    weight_to_update = torch.load(EDO_CHECKPOINT_PATH, weights_only=True)\n",
    "    for w in weight_to_update.keys():\n",
    "        if edo_model.state_dict().get(w) is not None:\n",
    "            edo_model.state_dict()[w].data.copy_(weight_to_update[w])\n",
    "\n",
    "                \n",
    "edo_model = edo_model.to(device)\n",
    "edo_model = edo_model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "59a6702d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from thop import clever_format, profile\n",
    "import tqdm \n",
    "input_tensor = torch.randn(64, 3, 224, 224).to(device)\n",
    "fl = 0\n",
    "\n",
    "with torch.no_grad():\n",
    "    #for b in tqdm.tqdm(range(input_tensor.shape[0]), total=input_tensor.shape[0]):\n",
    "    input_t = input_tensor#[b:b+1]\n",
    "    \n",
    "    flops, params = profile(edo_model, verbose=False, inputs=(input_t,))\n",
    "    fl +=flops\n",
    "\n",
    "fl_readable, params_readable = clever_format([fl, params], \"%.3f\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "ebb7c852",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(733694263296.0, '1.847M')"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fl, params_readable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "016cc267",
   "metadata": {},
   "outputs": [],
   "source": [
    "with torch.no_grad():\n",
    "    input_tensor = torch.randn(1000, 3, 224, 224).to(device)\n",
    "    flops, params = profile(model, verbose=False, inputs=(input_tensor,))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "8fe87296",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('16.863T', '86.237M')"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clever_format([flops, params], \"%.3f\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eafe6e9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "(16.863-5.732)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "graphocr",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
